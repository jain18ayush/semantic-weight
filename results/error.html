
            <!DOCTYPE html>
            <html>
            <head><title>Pipeline Error</title></head>
            <body>
                <h1>Pipeline Error</h1>
                <p>An error occurred while running the pipeline:</p>
                <pre>Failed to load any Vision-Language model. Original error: llava-hf/llava-1.5-7b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`, Fallback error: llava-hf/llava-1.5-13b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`</pre>
                <pre>Traceback (most recent call last):
  File "/Users/ayushjain/Development/Interp/semantic-weight/.venv/lib/python3.13/site-packages/huggingface_hub/utils/_http.py", line 409, in hf_raise_for_status
    response.raise_for_status()
    ~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "/Users/ayushjain/Development/Interp/semantic-weight/.venv/lib/python3.13/site-packages/requests/models.py", line 1024, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 404 Client Error: Not Found for url: https://huggingface.co/llava-hf/llava-1.5-7b/resolve/main/processor_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/Users/ayushjain/Development/Interp/semantic-weight/.venv/lib/python3.13/site-packages/transformers/utils/hub.py", line 342, in cached_file
    resolved_file = hf_hub_download(
        path_or_repo_id,
    ...<10 lines>...
        local_files_only=local_files_only,
    )
  File "/Users/ayushjain/Development/Interp/semantic-weight/.venv/lib/python3.13/site-packages/huggingface_hub/utils/_validators.py", line 114, in _inner_fn
    return fn(*args, **kwargs)
  File "/Users/ayushjain/Development/Interp/semantic-weight/.venv/lib/python3.13/site-packages/huggingface_hub/file_download.py", line 862, in hf_hub_download
    return _hf_hub_download_to_cache_dir(
        # Destination
    ...<14 lines>...
        force_download=force_download,
    )
  File "/Users/ayushjain/Development/Interp/semantic-weight/.venv/lib/python3.13/site-packages/huggingface_hub/file_download.py", line 969, in _hf_hub_download_to_cache_dir
    _raise_on_head_call_error(head_call_error, force_download, local_files_only)
    ~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/ayushjain/Development/Interp/semantic-weight/.venv/lib/python3.13/site-packages/huggingface_hub/file_download.py", line 1486, in _raise_on_head_call_error
    raise head_call_error
  File "/Users/ayushjain/Development/Interp/semantic-weight/.venv/lib/python3.13/site-packages/huggingface_hub/file_download.py", line 1376, in _get_metadata_or_catch_error
    metadata = get_hf_file_metadata(
        url=url, proxies=proxies, timeout=etag_timeout, headers=headers, token=token
    )
  File "/Users/ayushjain/Development/Interp/semantic-weight/.venv/lib/python3.13/site-packages/huggingface_hub/utils/_validators.py", line 114, in _inner_fn
    return fn(*args, **kwargs)
  File "/Users/ayushjain/Development/Interp/semantic-weight/.venv/lib/python3.13/site-packages/huggingface_hub/file_download.py", line 1296, in get_hf_file_metadata
    r = _request_wrapper(
        method="HEAD",
    ...<5 lines>...
        timeout=timeout,
    )
  File "/Users/ayushjain/Development/Interp/semantic-weight/.venv/lib/python3.13/site-packages/huggingface_hub/file_download.py", line 280, in _request_wrapper
    response = _request_wrapper(
        method=method,
    ...<2 lines>...
        **params,
    )
  File "/Users/ayushjain/Development/Interp/semantic-weight/.venv/lib/python3.13/site-packages/huggingface_hub/file_download.py", line 304, in _request_wrapper
    hf_raise_for_status(response)
    ~~~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/Users/ayushjain/Development/Interp/semantic-weight/.venv/lib/python3.13/site-packages/huggingface_hub/utils/_http.py", line 458, in hf_raise_for_status
    raise _format(RepositoryNotFoundError, message, response) from e
huggingface_hub.errors.RepositoryNotFoundError: 404 Client Error. (Request ID: Root=1-67bfea6a-46e338a92bd0c5e911245c62;b629dbe9-23ed-4479-ab94-2ab31c6b1e37)

Repository Not Found for url: https://huggingface.co/llava-hf/llava-1.5-7b/resolve/main/processor_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/Users/ayushjain/Development/Interp/semantic-weight/autointerp.py", line 85, in setup_vllm
    processor = AutoProcessor.from_pretrained(
        model_name,
    ...<2 lines>...
        token='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'
    )
  File "/Users/ayushjain/Development/Interp/semantic-weight/.venv/lib/python3.13/site-packages/transformers/models/auto/processing_auto.py", line 260, in from_pretrained
    processor_config_file = get_file_from_repo(
        pretrained_model_name_or_path, PROCESSOR_NAME, **get_file_from_repo_kwargs
    )
  File "/Users/ayushjain/Development/Interp/semantic-weight/.venv/lib/python3.13/site-packages/transformers/utils/hub.py", line 496, in get_file_from_repo
    return cached_file(
        path_or_repo_id=path_or_repo,
    ...<11 lines>...
        _raise_exceptions_for_connection_errors=False,
    )
  File "/Users/ayushjain/Development/Interp/semantic-weight/.venv/lib/python3.13/site-packages/transformers/utils/hub.py", line 365, in cached_file
    raise EnvironmentError(
    ...<4 lines>...
    ) from e
OSError: llava-hf/llava-1.5-7b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/ayushjain/Development/Interp/semantic-weight/.venv/lib/python3.13/site-packages/huggingface_hub/utils/_http.py", line 409, in hf_raise_for_status
    response.raise_for_status()
    ~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "/Users/ayushjain/Development/Interp/semantic-weight/.venv/lib/python3.13/site-packages/requests/models.py", line 1024, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 404 Client Error: Not Found for url: https://huggingface.co/llava-hf/llava-1.5-13b/resolve/main/processor_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/Users/ayushjain/Development/Interp/semantic-weight/.venv/lib/python3.13/site-packages/transformers/utils/hub.py", line 342, in cached_file
    resolved_file = hf_hub_download(
        path_or_repo_id,
    ...<10 lines>...
        local_files_only=local_files_only,
    )
  File "/Users/ayushjain/Development/Interp/semantic-weight/.venv/lib/python3.13/site-packages/huggingface_hub/utils/_validators.py", line 114, in _inner_fn
    return fn(*args, **kwargs)
  File "/Users/ayushjain/Development/Interp/semantic-weight/.venv/lib/python3.13/site-packages/huggingface_hub/file_download.py", line 862, in hf_hub_download
    return _hf_hub_download_to_cache_dir(
        # Destination
    ...<14 lines>...
        force_download=force_download,
    )
  File "/Users/ayushjain/Development/Interp/semantic-weight/.venv/lib/python3.13/site-packages/huggingface_hub/file_download.py", line 969, in _hf_hub_download_to_cache_dir
    _raise_on_head_call_error(head_call_error, force_download, local_files_only)
    ~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/ayushjain/Development/Interp/semantic-weight/.venv/lib/python3.13/site-packages/huggingface_hub/file_download.py", line 1486, in _raise_on_head_call_error
    raise head_call_error
  File "/Users/ayushjain/Development/Interp/semantic-weight/.venv/lib/python3.13/site-packages/huggingface_hub/file_download.py", line 1376, in _get_metadata_or_catch_error
    metadata = get_hf_file_metadata(
        url=url, proxies=proxies, timeout=etag_timeout, headers=headers, token=token
    )
  File "/Users/ayushjain/Development/Interp/semantic-weight/.venv/lib/python3.13/site-packages/huggingface_hub/utils/_validators.py", line 114, in _inner_fn
    return fn(*args, **kwargs)
  File "/Users/ayushjain/Development/Interp/semantic-weight/.venv/lib/python3.13/site-packages/huggingface_hub/file_download.py", line 1296, in get_hf_file_metadata
    r = _request_wrapper(
        method="HEAD",
    ...<5 lines>...
        timeout=timeout,
    )
  File "/Users/ayushjain/Development/Interp/semantic-weight/.venv/lib/python3.13/site-packages/huggingface_hub/file_download.py", line 280, in _request_wrapper
    response = _request_wrapper(
        method=method,
    ...<2 lines>...
        **params,
    )
  File "/Users/ayushjain/Development/Interp/semantic-weight/.venv/lib/python3.13/site-packages/huggingface_hub/file_download.py", line 304, in _request_wrapper
    hf_raise_for_status(response)
    ~~~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "/Users/ayushjain/Development/Interp/semantic-weight/.venv/lib/python3.13/site-packages/huggingface_hub/utils/_http.py", line 458, in hf_raise_for_status
    raise _format(RepositoryNotFoundError, message, response) from e
huggingface_hub.errors.RepositoryNotFoundError: 404 Client Error. (Request ID: Root=1-67bfea6a-17659810464d5ef56f39cf9d;f28efa53-5932-4290-b1a6-bec5c7cd3ca2)

Repository Not Found for url: https://huggingface.co/llava-hf/llava-1.5-13b/resolve/main/processor_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/Users/ayushjain/Development/Interp/semantic-weight/autointerp.py", line 125, in setup_vllm
    processor = AutoProcessor.from_pretrained(
        model_name,
    ...<2 lines>...
        token='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'
    )
  File "/Users/ayushjain/Development/Interp/semantic-weight/.venv/lib/python3.13/site-packages/transformers/models/auto/processing_auto.py", line 260, in from_pretrained
    processor_config_file = get_file_from_repo(
        pretrained_model_name_or_path, PROCESSOR_NAME, **get_file_from_repo_kwargs
    )
  File "/Users/ayushjain/Development/Interp/semantic-weight/.venv/lib/python3.13/site-packages/transformers/utils/hub.py", line 496, in get_file_from_repo
    return cached_file(
        path_or_repo_id=path_or_repo,
    ...<11 lines>...
        _raise_exceptions_for_connection_errors=False,
    )
  File "/Users/ayushjain/Development/Interp/semantic-weight/.venv/lib/python3.13/site-packages/transformers/utils/hub.py", line 365, in cached_file
    raise EnvironmentError(
    ...<4 lines>...
    ) from e
OSError: llava-hf/llava-1.5-13b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/ayushjain/Development/Interp/semantic-weight/autointerp.py", line 1223, in main
    vllm_model, processor, tokenizer = setup_vllm()
                                       ~~~~~~~~~~^^
  File "/Users/ayushjain/Development/Interp/semantic-weight/autointerp.py", line 149, in setup_vllm
    raise RuntimeError(f"Failed to load any Vision-Language model. Original error: {str(e)}, Fallback error: {str(e2)}")
RuntimeError: Failed to load any Vision-Language model. Original error: llava-hf/llava-1.5-7b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`, Fallback error: llava-hf/llava-1.5-13b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`
</pre>
            </body>
            </html>
            